{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"video_capture.ipynb","provenance":[],"mount_file_id":"1SrIA7M7s6mkT305f0oWWHBMDLY7YBrF9","authorship_tag":"ABX9TyOtoN3okM5kFOpxM48J5ew8"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"dgxHJgM25l7i","executionInfo":{"status":"ok","timestamp":1647064061843,"user_tz":-330,"elapsed":4209,"user":{"displayName":"JADHAV SHUBHAM GANPAT","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08170360707749377626"}}},"outputs":[],"source":["from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n","from tensorflow.keras.preprocessing.image import img_to_array\n","from tensorflow.keras.models import load_model\n","from imutils.video import VideoStream\n","import numpy as np\n","import imutils\n","import time\n","import cv2\n","import os"]},{"cell_type":"code","source":["def detect_and_predict_mask(frame, faceNet, maskNet):\n","\t# grab the dimensions of the frame and then construct a blob\n","\t# from it\n","\t(h, w) = frame.shape[:2]\n","\tblob = cv2.dnn.blobFromImage(frame, 1.0, (224, 224),\n","\t\t(104.0, 177.0, 123.0))\n","\n","\t# pass the blob through the network and obtain the face detections\n","\tfaceNet.setInput(blob)\n","\tdetections = faceNet.forward()\n","\tprint(detections.shape)\n","\n","\t# initialize our list of faces, their corresponding locations,\n","\t# and the list of predictions from our face mask network\n","\tfaces = []\n","\tlocs = []\n","\tpreds = []\n","\n","\t# loop over the detections\n","\tfor i in range(0, detections.shape[2]):\n","\t\t# extract the confidence (i.e., probability) associated with\n","\t\t# the detection\n","\t\tconfidence = detections[0, 0, i, 2]\n","\n","\t\t# filter out weak detections by ensuring the confidence is\n","\t\t# greater than the minimum confidence\n","\t\tif confidence > 0.5:\n","\t\t\t# compute the (x, y)-coordinates of the bounding box for\n","\t\t\t# the object\n","\t\t\tbox = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n","\t\t\t(startX, startY, endX, endY) = box.astype(\"int\")\n","\n","\t\t\t# ensure the bounding boxes fall within the dimensions of\n","\t\t\t# the frame\n","\t\t\t(startX, startY) = (max(0, startX), max(0, startY))\n","\t\t\t(endX, endY) = (min(w - 1, endX), min(h - 1, endY))\n","\n","\t\t\t# extract the face ROI, convert it from BGR to RGB channel\n","\t\t\t# ordering, resize it to 224x224, and preprocess it\n","\t\t\tface = frame[startY:endY, startX:endX]\n","\t\t\tface = cv2.cvtColor(face, cv2.COLOR_BGR2RGB)\n","\t\t\tface = cv2.resize(face, (224, 224))\n","\t\t\tface = img_to_array(face)\n","\t\t\tface = preprocess_input(face)\n","\n","\t\t\t# add the face and bounding boxes to their respective\n","\t\t\t# lists\n","\t\t\tfaces.append(face)\n","\t\t\tlocs.append((startX, startY, endX, endY))\n","\n","\t# only make a predictions if at least one face was detected\n","\tif len(faces) > 0:\n","\t\t# for faster inference we'll make batch predictions on *all*\n","\t\t# faces at the same time rather than one-by-one predictions\n","\t\t# in the above `for` loop\n","\t\tfaces = np.array(faces, dtype=\"float32\")\n","\t\tpreds = maskNet.predict(faces, batch_size=32)\n","\n","\t# return a 2-tuple of the face locations and their corresponding\n","\t# locations\n","\treturn (locs, preds)\n","\n","# load our serialized face detector model from disk\n","prototxtPath = r\"/content/drive/MyDrive/intern dl/face dtectection/deploy.prototxt\"\n","weightsPath = r\"/content/drive/MyDrive/intern dl/face dtectection/res10_300x300_ssd_iter_140000.caffemodel\"\n","faceNet = cv2.dnn.readNet(prototxtPath, weightsPath)\n","\n","# load the face mask detector model from disk\n","maskNet = load_model(\"/content/drive/MyDrive/intern dl/face dtectection/model1.h5\")\n","\n","# initialize the video stream\n","print(\"[INFO] starting video stream...\")\n","vs = VideoStream(src=0).start()\n","\n","# loop over the frames from the video stream\n","while True:\n","\t# grab the frame from the threaded video stream and resize it\n","\t# to have a maximum width of 400 pixels\n","\tframe = vs.read()\n","\tframe = imutils.resize(frame, width=400)\n","\n","\t# detect faces in the frame and determine if they are wearing a\n","\t# face mask or not\n","\t(locs, preds) = detect_and_predict_mask(frame, faceNet, maskNet)\n","\n","\t# loop over the detected face locations and their corresponding\n","\t# locations\n","\tfor (box, pred) in zip(locs, preds):\n","\t\t# unpack the bounding box and predictions\n","\t\t(startX, startY, endX, endY) = box\n","\t\t(mask, withoutMask) = pred\n","\n","\t\t# determine the class label and color we'll use to draw\n","\t\t# the bounding box and text\n","\t\tlabel = \"Mask\" if mask > withoutMask else \"No Mask\"\n","\t\tcolor = (0, 255, 0) if label == \"Mask\" else (0, 0, 255)\n","\n","\t\t# include the probability in the label\n","\t\tlabel = \"{}: {:.2f}%\".format(label, max(mask, withoutMask) * 100)\n","\n","\t\t# display the label and bounding box rectangle on the output\n","\t\t# frame\n","\t\tcv2.putText(frame, label, (startX, startY - 10),\n","\t\t\tcv2.FONT_HERSHEY_SIMPLEX, 0.45, color, 2)\n","\t\tcv2.rectangle(frame, (startX, startY), (endX, endY), color, 2)\n","\n","\t# show the output frame\n","\tcv2.imshow(\"Frame\", frame)\n","\tkey = cv2.waitKey(1) & 0xFF\n","\n","\t# if the `q` key was pressed, break from the loop\n","\tif key == ord(\"q\"):\n","\t\tbreak\n","\n","# do a bit of cleanup\n","cv2.destroyAllWindows()\n","vs.stop()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":374},"id":"YdvZANt25yNB","executionInfo":{"status":"error","timestamp":1647065966701,"user_tz":-330,"elapsed":4180,"user":{"displayName":"JADHAV SHUBHAM GANPAT","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08170360707749377626"}},"outputId":"07fcc4e9-b233-45b3-9c71-11a330b6946e"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["[INFO] starting video stream...\n"]},{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-dac1871045a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;31m# to have a maximum width of 400 pixels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m400\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;31m# detect faces in the frame and determine if they are wearing a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/imutils/convenience.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(image, width, height, inter)\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;31m# grab the image size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m     \u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;31m# if both the width and height are None, then return the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"IuAmPgGz6pNI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"JphIPNHl52Bv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"C1dl7x9852HJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"tqcNCC8p52JG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"BXgQxQk552Kt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"mFQkMWHq52NU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"SLhRnKbY52Pa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"CIdkROYE52Rf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"zktT02wh52UB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"-av3W8gg52Wc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"Hlx6tx3g52Yp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"v4FBRwDP52aI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"vg-JCNE752cq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"HBklctwr52ev"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"sG1boixn52iQ"},"execution_count":null,"outputs":[]}]}